{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "\n",
    "try:\n",
    "    import face_recognition_models\n",
    "except Exception:\n",
    "    print(\"Please install `face_recognition_models` with this command before using `face_recognition`:\\n\")\n",
    "    print(\"pip install git+https://github.com/ageitgey/face_recognition_models\")\n",
    "    quit()\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "predictor_68_point_model = face_recognition_models.pose_predictor_model_location()\n",
    "# def pose_predictor_model_location():\n",
    "#    return resource_filename(__name__, \"models/shape_predictor_68_face_landmarks.dat\")\n",
    "pose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model) # 정의되어있는 point를 return\n",
    "\n",
    "predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\n",
    "# def pose_predictor_five_point_model_location():\n",
    "#    return resource_filename(__name__, \"models/shape_predictor_5_face_landmarks.dat\")\n",
    "pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model) # 정의되어있는 point를 return\n",
    "\n",
    "cnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\n",
    "# def cnn_face_detector_model_location():\n",
    "#    return resource_filename(__name__, \"models/mmod_human_face_detector.dat\") filesystem path를 return\n",
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model) # file에서 model 불러와서 img detect\n",
    "\n",
    "face_recognition_model = face_recognition_models.face_recognition_model_location()\n",
    "# def face_recognition_model_location():\n",
    "#    return resource_filename(__name__, \"models/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "face_encoder = dlib.face_recognition_model_v1(face_recognition_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rect_to_css(rect): # 사진의 좌표를 tuple로 return\n",
    "    \"\"\"\n",
    "    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n",
    "\n",
    "    :param rect: a dlib 'rect' object\n",
    "    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n",
    "    \"\"\"\n",
    "    return rect.top(), rect.right(), rect.bottom(), rect.left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _css_to_rect(css):\n",
    "    \"\"\"\n",
    "    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\n",
    "\n",
    "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
    "    :return: a dlib `rect` object\n",
    "    \"\"\"\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2]) # tuple을 rect객체로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trim_css_to_bounds(css, image_shape): # 이미지 크기랑 detect된 얼굴 좌표 비교해서 작은좌표 return\n",
    "    \"\"\"\n",
    "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
    "\n",
    "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
    "    :param image_shape: numpy shape of the image array\n",
    "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
    "    \"\"\"\n",
    "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_distance(face_encodings, face_to_compare):\n",
    "    \"\"\"\n",
    "    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n",
    "    for each comparison face. The distance tells you how similar the faces are.\n",
    "\n",
    "    :param faces: List of face encodings to compare\n",
    "    :param face_to_compare: A face encoding to compare against\n",
    "    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n",
    "    \"\"\"\n",
    "    if len(face_encodings) == 0:\n",
    "        return np.empty((0))\n",
    "\n",
    "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_file(file, mode='RGB'):\n",
    "    # file open해서 array로 return\n",
    "    \"\"\"\n",
    "    Loads an image file (.jpg, .png, etc) into a numpy array\n",
    "\n",
    "    :param file: image file name or file object to load\n",
    "    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n",
    "    :return: image contents as numpy array\n",
    "    \"\"\"\n",
    "    im = PIL.Image.open(file)\n",
    "    if mode:\n",
    "        im = im.convert(mode)\n",
    "    return np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "    \"\"\"\n",
    "    Returns an array of bounding boxes of human faces in a image\n",
    "\n",
    "    :param img: An image (as a numpy array)\n",
    "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
    "    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n",
    "                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n",
    "    :return: A list of dlib 'rect' objects of found face locations\n",
    "    \"\"\"\n",
    "    \n",
    "    # cnn(Convolutional Neural Networks - convolution(기존영상처리필터)과 신경망 결합 성능좋음)\n",
    "    if model == \"cnn\":\n",
    "        return cnn_face_detector(img, number_of_times_to_upsample) # 불러온 모델로 img에서 얼굴 검출\n",
    "    else:\n",
    "        return face_detector(img, number_of_times_to_upsample) # img에서 얼굴 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "    \"\"\"\n",
    "    Returns an array of bounding boxes of human faces in a image\n",
    "\n",
    "    :param img: An image (as a numpy array)\n",
    "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
    "    # 사진 size를 키우는것 -> 작은얼굴도 찾을수 있도록\n",
    "    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n",
    "                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n",
    "    #face detection model 뭐로 할지\n",
    "    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n",
    "    \"\"\"\n",
    "    # img에서 얼굴검출하고, img안에 detect된 얼굴이 있는지 다시 확인하고 좌표들 list로 return\n",
    "    if model == \"cnn\":\n",
    "        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n",
    "    else:\n",
    "        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\n",
    "    \"\"\"\n",
    "    Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\n",
    "\n",
    "    :param img: A list of images (each as a numpy array)\n",
    "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
    "    :return: A list of dlib 'rect' objects of found face locations\n",
    "    \"\"\"\n",
    "    return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n",
    "    \"\"\"\n",
    "    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n",
    "    If you are using a GPU, this can give you much faster results since the GPU\n",
    "    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n",
    "\n",
    "    :param img: A list of images (each as a numpy array)\n",
    "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
    "    :param batch_size: How many images to include in each GPU processing batch.\n",
    "    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n",
    "    \"\"\"\n",
    "    def convert_cnn_detections_to_css(detections):\n",
    "        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n",
    "\n",
    "    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n",
    "\n",
    "    return list(map(convert_cnn_detections_to_css, raw_detections_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
    "    if face_locations is None: # face_locations가 None이면 face location을 찾아옴, None아니면 얼굴 좌표를 rect객체로\n",
    "        face_locations = _raw_face_locations(face_image)\n",
    "    else:\n",
    "        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\n",
    "\n",
    "    pose_predictor = pose_predictor_68_point\n",
    "\n",
    "    if model == \"small\":\n",
    "        pose_predictor = pose_predictor_5_point\n",
    "\n",
    "    return [pose_predictor(face_image, face_location) for face_location in face_locations] # 이미지에 point 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
    "    \"\"\"\n",
    "    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n",
    "\n",
    "    :param face_image: image to search\n",
    "    :param face_locations: Optionally provide a list of face locations to check.\n",
    "    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n",
    "    :return: A list of dicts of face feature locations (eyes, nose, etc)\n",
    "    \"\"\"\n",
    "    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n",
    "    # \n",
    "    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n",
    "\n",
    "    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n",
    "    if model == 'large':\n",
    "        return [{\n",
    "            \"chin\": points[0:17],\n",
    "            \"left_eyebrow\": points[17:22],\n",
    "            \"right_eyebrow\": points[22:27],\n",
    "            \"nose_bridge\": points[27:31],\n",
    "            \"nose_tip\": points[31:36],\n",
    "            \"left_eye\": points[36:42],\n",
    "            \"right_eye\": points[42:48],\n",
    "            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n",
    "            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n",
    "        } for points in landmarks_as_tuples]\n",
    "    elif model == 'small':\n",
    "        return [{\n",
    "            \"nose_tip\": [points[4]],\n",
    "            \"left_eye\": points[2:4],\n",
    "            \"right_eye\": points[0:2],\n",
    "        } for points in landmarks_as_tuples]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n",
    "    \"\"\"\n",
    "    Given an image, return the 128-dimension face encoding for each face in the image.\n",
    "\n",
    "    :param face_image: The image that contains one or more faces\n",
    "    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n",
    "    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n",
    "    :return: A list of 128-dimensional face encodings (one for each face in the image)\n",
    "    \"\"\"\n",
    "    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=\"small\")\n",
    "    # 얼굴들을 128크기의 vector로 변환 & jitter>1이면 resampling을 몇번할건지\n",
    "    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n",
    "    \"\"\"\n",
    "    Compare a list of face encodings against a candidate encoding to see if they match.\n",
    "\n",
    "    :param known_face_encodings: A list of known face encodings\n",
    "    :param face_encoding_to_check: A single face encoding to compare against the list\n",
    "    :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\n",
    "    :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\n",
    "    \"\"\"\n",
    "    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance) # known_face와 frame의 distance가 <=0.6인지아닌지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 1 face(s) in this photograph.\n",
      "A face is located at pixel location Top: 80, Left: 79, Bottom: 187, Right: 187\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"/home/qisens/hj/face_recognition_1/examples/dataset/irene.png\")\n",
    "\n",
    "# Find all the faces in the image using the default HOG-based model.\n",
    "# This method is fairly accurate, but not as accurate as the CNN model and not GPU accelerated.\n",
    "# See also: find_faces_in_picture_cnn.py\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "\n",
    "for face_location in face_locations:\n",
    "\n",
    "    # Print the location of each face in this image\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "    face_image = image[top:bottom, left:right]\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
